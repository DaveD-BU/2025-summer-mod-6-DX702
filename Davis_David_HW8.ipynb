{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a300f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b83c1e",
   "metadata": {},
   "source": [
    "## Question 1 & 2\n",
    "\n",
    "Using homework_8.1.csv, find the Average treatment effect with inverse probability weighting. Then, include your code and a written explanation of your work (mentioning any choices or strategies you made in writing the code) in your homework reflection.  \n",
    "\n",
    "Here are some steps to follow: \n",
    "\n",
    "* Estimate the propensity scores using logistic regression. Fit the model so that the Z values predict ÔªøXÔªø. \n",
    "\n",
    "* Use the model to predict the propensity scores (e.g., using predict_proba if you are using sklearn). \n",
    "\n",
    "* Calculate inverse probability weights (Ôªø1 over PÔªø for ÔªøX equals 1Ôªø and Ôªøfraction numerator 1 over denominator 1 minus P end fractionÔªø for ÔªøX equals 0Ôªø). \n",
    "\n",
    "* Estimate the average treatment effect (the Y difference between ÔªøX equals 1Ôªø and ÔªøX equals 0Ôªø, using the appropriate weights for each). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e59bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('homework_8.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8929c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Estimate propensity scores P(X=1 | Z)\n",
    "logit = LogisticRegression()\n",
    "logit.fit(df[['Z']], df['X'])\n",
    "propensity_scores = logit.predict_proba(df[['Z']])[:, 1]  # probability of X=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6526b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate inverse probability weights\n",
    "df['weights'] = np.where(df['X'] == 1, 1 / propensity_scores, 1 / (1 - propensity_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df9f6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute weighted average outcomes\n",
    "treated = df[df['X'] == 1]\n",
    "control = df[df['X'] == 0]\n",
    "# Weighted means\n",
    "mean_treated = np.sum(treated['Y'] * treated['weights']) / np.sum(treated['weights'])\n",
    "mean_control = np.sum(control['Y'] * control['weights']) / np.sum(control['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bca7dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (ATE) using IPW: 2.2743\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Estimate Average Treatment Effect (ATE)\n",
    "ate = mean_treated - mean_control\n",
    "\n",
    "print(f\"Average Treatment Effect (ATE) using IPW: {ate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c5c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propensity scores for the first three items: [0.84011371 0.58464597 0.71108245]\n"
     ]
    }
   ],
   "source": [
    "#The First 3 propensity scores\n",
    "propensity_scores_first_three = propensity_scores[:3]\n",
    "print(\"Propensity scores for the first three items:\", propensity_scores_first_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eb405",
   "metadata": {},
   "source": [
    "## Question 3 & 4\n",
    "\n",
    "Using homework_8.2.csv, match all treated items to the single nearest untreated item using the Mahalanobis distance. (Do this with replacement ‚Äî the same untreated item can be used again.) \n",
    "\n",
    "* Use the Mahalanobis function from scipy.spatial.distance \n",
    "\n",
    "* For the inverse covariance matrix, use all ÔªøZ 1Ôªø values and all ÔªøZ 2Ôªø values, make them into a Ôªø2 x NÔªø matrix, find its Ôªø2 x 2Ôªø covariance, and invert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5950e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('homework_8.2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ca4aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split treated and control\n",
    "treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "control = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Extract covariates\n",
    "covariates = ['Z1', 'Z2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcaaa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inverse covariance matrix (2 x 2) using all Z1, Z2\n",
    "Z_all = df[covariates].values\n",
    "cov_matrix = np.cov(Z_all, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a606c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each treated, find nearest control\n",
    "matches = []\n",
    "for i, treated_row in treated.iterrows():\n",
    "    treated_point = treated_row[covariates].values\n",
    "    dists = control[covariates].apply(\n",
    "        lambda row: distance.mahalanobis(treated_point, row.values, inv_cov_matrix), axis=1)\n",
    "    \n",
    "    nearest_idx = dists.idxmin()\n",
    "    nearest_control = control.loc[nearest_idx]\n",
    "    nearest_distance = dists[nearest_idx]\n",
    "    \n",
    "    matches.append({\n",
    "        'treated_index': treated_row.name,\n",
    "        'treated_Z1': treated_row['Z1'],\n",
    "        'treated_Z2': treated_row['Z2'],\n",
    "        'control_index': nearest_idx,\n",
    "        'control_Z1': nearest_control['Z1'],\n",
    "        'control_Z2': nearest_control['Z2'],\n",
    "        'mahalanobis_distance': nearest_distance\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039d2fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated ATE (Average Treatment Effect) from matched pairs: 3.4377\n"
     ]
    }
   ],
   "source": [
    "# Get Y values for treated and matched controls\n",
    "treated_Ys = treated['Y']\n",
    "matched_control_Ys = control.loc[matches_df['control_index']]['Y'].values\n",
    "\n",
    "# Calculate differences\n",
    "diffs = treated_Ys.values - matched_control_Ys\n",
    "\n",
    "# Calculate ATE\n",
    "ate = np.mean(diffs)\n",
    "\n",
    "print(f\"\\nEstimated ATE (Average Treatment Effect) from matched pairs: {ate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52ff329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matches DataFrame:\n",
      "     treated_index  treated_Z1  treated_Z2  control_index  control_Z1  \\\n",
      "241            241    2.696224    0.538155            217    1.519995   \n",
      "429            429    2.594425    2.893138             77    1.929532   \n",
      "352            352    2.497200    2.639007             43    1.895889   \n",
      "27              27   -0.028182    3.142793            117   -0.637437   \n",
      "147            147    2.303917    2.464578            481    1.325014   \n",
      "\n",
      "     control_Z2  mahalanobis_distance  \n",
      "241   -1.282208              1.383005  \n",
      "429    3.355691              1.326817  \n",
      "352    1.095807              1.164020  \n",
      "27     1.683363              1.089164  \n",
      "147    1.169978              1.053704  \n"
     ]
    }
   ],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "#order by furthest mahalanobis_distance \n",
    "matches_df = matches_df.sort_values(by='mahalanobis_distance', ascending=False)\n",
    "print(\"\\nMatches DataFrame:\")\n",
    "print(matches_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c8a74",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them.\n",
    "\n",
    "### Obstacles / Challenges\n",
    "* **Understanding the data structure**\n",
    "    The first challenge was making sure the correct variables were used, especially ensuring we correctly separated treated vs. control and identified the covariates ùëç1, ùëç2 for distance calculations.\n",
    "* **Mahalanobis distance implementation**\n",
    "    Mahalanobis distance needs the inverse covariance matrix of the covariates. Remembering to calculate it properly (across all data, not just treated or controls) and understanding the scipy function parameters required careful attention.\n",
    "* **Matching with replacement logic**\n",
    "    It was important to allow controls to be re-used across treated units, rather than doing a one-to-one matching without replacement, which is more complex.\n",
    "* **Keeping track of indices after filtering**\n",
    "    Since pandas resets indices when you filter (like with .reset_index()), matching back to the original dataframe required careful handling to avoid misreporting which control matched.\n",
    "\n",
    "\n",
    "\n",
    "### Insights\n",
    "\n",
    "* **Propensity score weighting is sensitive to model specification**\n",
    "    I learned that estimating good propensity scores depends on correctly specifying the model of ùëã on covariates. Poor models can lead to extreme weights and unstable ATE estimates.\n",
    "\n",
    "* **Common support matters for causal inference**\n",
    "    Finding the treated units with poor matches (least common support) helps reveal where the treatment and control groups don‚Äôt overlap well, which is a key assumption for causal interpretation.\n",
    "\n",
    "* **Mahalanobis distance gives a multidimensional notion of similarity**\n",
    "    Unlike just matching on raw differences, Mahalanobis distance accounts for the variance and correlation between covariates, making it a more robust matching method when multiple covariates are involved.\n",
    "* **Careful data handling matters as much as the math**\n",
    "    A lot of the challenge was not just the formulas but handling the dataframe operations cleanly ‚Äî especially tracking row indices, merging results, and ensuring distances matched the right pairs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
