{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58805c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe88ada",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Suppose that a process can be modeled via linear regression: \n",
    "\n",
    "```python\n",
    "W = np.random.normal(1, 0, (1000,))\n",
    "X = W + np.random.normal(1, 0, (1000,)) \n",
    "Z = np.random.normal(1, 0, (1000,)) \n",
    "Y = X + Z + W + np.random.normal(1, 0, (1000,))\n",
    "```\n",
    "\n",
    "Which is closest to the correlation of ï»¿Xï»¿ with the error term in the equation for ï»¿Yï»¿? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b231ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "W = np.random.normal(1, 0, (1000,))\n",
    "X = W + np.random.normal(1, 0, (1000,))\n",
    "Z = np.random.normal(1, 0, (1000,))\n",
    "Y = X + Z + W + np.random.normal(1, 0, (1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f322b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X    2.810072e-17\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sm.add_constant(pd.DataFrame({\"X\": X}))\n",
    "results = sm.OLS(Y, df).fit()\n",
    "results.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e89820f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the variance of W\n",
    "W_var = np.var(W)\n",
    "W_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed43b6",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "If ï»¿Yï»¿ is written as depending on ï»¿Xï»¿ and ï»¿Zï»¿ only, ï»¿Wï»¿ is part of the error term. Which, then, is closest to the expected correlation of ï»¿Xï»¿ with the error term in the equation for ï»¿Yï»¿? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50b5e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between X and residuals (error): 0.0000\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "\n",
    "# Generate W, epsilon_X, epsilon_Y, and Z\n",
    "W = np.random.normal(0, 1, n)\n",
    "epsilon_X = np.random.normal(0, 1, n)\n",
    "epsilon_Y = np.random.normal(0, 1, n)\n",
    "Z = np.random.normal(0, 1, n)\n",
    "\n",
    "# Generate X and Y\n",
    "X = W + epsilon_X\n",
    "Y = X + Z + W + epsilon_Y  # True model includes W\n",
    "\n",
    "# Regress Y on X and Z (omitting W)\n",
    "X_Z = np.column_stack((X, Z))\n",
    "X_Z = sm.add_constant(X_Z)  # Add intercept\n",
    "model = sm.OLS(Y, X_Z).fit()\n",
    "\n",
    "# Get residuals (error term)\n",
    "residuals = model.resid\n",
    "\n",
    "# Compute correlation between X and the error term\n",
    "correlation = np.corrcoef(X, residuals)[0, 1]\n",
    "\n",
    "print(f\"Correlation between X and residuals (error): {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd78ea6",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In the data frame for homework_7.1.csv, control for W by regressing ï»¿Yï»¿ on ï»¿Xï»¿ and ï»¿Zï»¿ at the following constant values of ï»¿Wï»¿: -1, 0, and 1. (You cannot literally use a constant value of ï»¿Wï»¿, of course, or you will have only one data point! How will you manage this?) The question is: Is the coefficient of ï»¿Xï»¿  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34a4abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   W approx    n   X coeff        p-value        RÂ²\n",
      "0        -1  488  0.857978   1.921114e-59  0.509544\n",
      "1         0  780  1.383211  9.340880e-188  0.697200\n",
      "2         1  455  1.958097  1.297296e-164  0.817845\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"homework_7.1.csv\")  # Replace with your actual file path or string\n",
    "\n",
    "# Define target W values and tolerance\n",
    "target_W_values = [-1, 0, 1]\n",
    "tolerance = 0.1\n",
    "\n",
    "results = []\n",
    "\n",
    "for w_val in target_W_values:\n",
    "    # Subset the data for W near the target value\n",
    "    df_subset = df[(df[\"W\"] >= w_val - tolerance) & (df[\"W\"] <= w_val + tolerance)]\n",
    "    \n",
    "    # Prepare regressors and response\n",
    "    X_vars = sm.add_constant(df_subset[[\"X\", \"Z\"]])\n",
    "    y_var = df_subset[\"Y\"]\n",
    "\n",
    "    # Run OLS regression\n",
    "    model = sm.OLS(y_var, X_vars).fit()\n",
    "\n",
    "    # Store the coefficient of X\n",
    "    results.append({\n",
    "        \"W approx\": w_val,\n",
    "        \"n\": len(df_subset),\n",
    "        \"X coeff\": model.params[\"X\"],\n",
    "        \"p-value\": model.pvalues[\"X\"],\n",
    "        \"RÂ²\": model.rsquared\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73902cdb",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "```python\n",
    "def make_error(corr_const, num): \n",
    " err = list() \n",
    " prev = np.random.normal(0, 1) \n",
    " for n in range(num): \n",
    "    prev = corr_const * prev + (1 - corr_const) * np.random.normal(0, 1) \n",
    "    err.append(prev) \n",
    " return np.array(err) \n",
    "```\n",
    "\n",
    "\n",
    "Create a linear regression model that uses this function as the error for both (a) the treatment, ï»¿Xï»¿, and (b) the outcome, ï»¿Yï»¿. (You can use random normal error for any other covariates, if you have them.) \n",
    "As corr_const increases from 0.2 to 0.5 to 0.8, find (i) the standard deviation of the estimate of the ï»¿Xï»¿ coefficient over many trials, and (ii) the mean of the standard error estimate of the ï»¿Xï»¿ coefficient over many trials. \n",
    "When corr_const increases, then: \n",
    "\n",
    "Hint: don't forget to include an intercept in your regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26efd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_error(corr_const, num): \n",
    " err = list() \n",
    " prev = np.random.normal(0, 1) \n",
    " for n in range(num): \n",
    "    prev = corr_const * prev + (1 - corr_const) * np.random.normal(0, 1) \n",
    "    err.append(prev) \n",
    " return np.array(err) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5af008af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   corr_const  std_dev_beta_hat  mean_std_error\n",
      "0         0.2          0.106568        0.101308\n",
      "1         0.5          0.133763        0.100808\n",
      "2         0.8          0.239446        0.100172\n"
     ]
    }
   ],
   "source": [
    "def run_simulation(corr_const, num_trials=1000, n=100):\n",
    "    beta_ests = []\n",
    "    se_ests = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        e_X = make_error(corr_const, n)\n",
    "        e_Y = make_error(corr_const, n)\n",
    "\n",
    "        X = 1 + e_X  # Include intercept-like component\n",
    "        Y = 2 + 3 * X + e_Y  # True model: Y = 2 + 3X + noise\n",
    "\n",
    "        # Regression\n",
    "        X_design = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X_design).fit()\n",
    "        \n",
    "        beta_ests.append(model.params[1])  # Coefficient for X\n",
    "        se_ests.append(model.bse[1])       # Standard error of X coefficient\n",
    "\n",
    "    # Compute summary statistics\n",
    "    std_beta = np.std(beta_ests)\n",
    "    mean_se = np.mean(se_ests)\n",
    "\n",
    "    return std_beta, mean_se\n",
    "\n",
    "# Run for different correlation constants\n",
    "results = []\n",
    "\n",
    "for corr in [0.2, 0.5, 0.8]:\n",
    "    std_b, mean_se = run_simulation(corr_const=corr)\n",
    "    results.append({\n",
    "        \"corr_const\": corr,\n",
    "        \"std_dev_beta_hat\": std_b,\n",
    "        \"mean_std_error\": mean_se\n",
    "    })\n",
    "\n",
    "# Show results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ed97b",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between $X$ and $Y$ is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770aa5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspecified model (without W):\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.746\n",
      "Model:                            OLS   Adj. R-squared:                  0.746\n",
      "Method:                 Least Squares   F-statistic:                     2932.\n",
      "Date:                Sat, 12 Jul 2025   Prob (F-statistic):          2.61e-299\n",
      "Time:                        10:56:24   Log-Likelihood:                -2255.6\n",
      "No. Observations:                1000   AIC:                             4515.\n",
      "Df Residuals:                     998   BIC:                             4525.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.1406      0.073     -1.923      0.055      -0.284       0.003\n",
      "x1             4.1140      0.076     54.150      0.000       3.965       4.263\n",
      "==============================================================================\n",
      "Omnibus:                        3.216   Durbin-Watson:                   1.946\n",
      "Prob(Omnibus):                  0.200   Jarque-Bera (JB):                3.110\n",
      "Skew:                          -0.106   Prob(JB):                        0.211\n",
      "Kurtosis:                       3.172   Cond. No.                         1.05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Correct model (with W):\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.957\n",
      "Model:                            OLS   Adj. R-squared:                  0.957\n",
      "Method:                 Least Squares   F-statistic:                 1.101e+04\n",
      "Date:                Sat, 12 Jul 2025   Prob (F-statistic):               0.00\n",
      "Time:                        10:56:24   Log-Likelihood:                -1371.3\n",
      "No. Observations:                1000   AIC:                             2749.\n",
      "Df Residuals:                     997   BIC:                             2763.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0529      0.030     -1.748      0.081      -0.112       0.006\n",
      "x1             1.9975      0.044     45.714      0.000       1.912       2.083\n",
      "x2             2.9653      0.043     69.635      0.000       2.882       3.049\n",
      "==============================================================================\n",
      "Omnibus:                        3.889   Durbin-Watson:                   1.838\n",
      "Prob(Omnibus):                  0.143   Jarque-Bera (JB):                3.505\n",
      "Skew:                          -0.081   Prob(JB):                        0.173\n",
      "Kurtosis:                       2.760   Cond. No.                         2.37\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# Generate W\n",
    "W = np.random.normal(0, 1, n)\n",
    "\n",
    "# Make X correlated with W\n",
    "X = 0.7 * W + np.sqrt(1 - 0.7**2) * np.random.normal(0, 1, n)\n",
    "\n",
    "# True coefficients\n",
    "beta_0 = 0\n",
    "beta_1 = 2  # effect of X\n",
    "beta_2 = 3  # effect of W\n",
    "\n",
    "# Generate Y\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "Y = beta_0 + beta_1 * X + beta_2 * W + epsilon\n",
    "\n",
    "# Regress Y on X (misspecified, omitting W)\n",
    "X_with_const = sm.add_constant(X)\n",
    "model_misspec = sm.OLS(Y, X_with_const).fit()\n",
    "print(\"Misspecified model (without W):\")\n",
    "print(model_misspec.summary())\n",
    "\n",
    "# Regress Y on X and W (correct)\n",
    "XW_with_const = sm.add_constant(np.column_stack((X, W)))\n",
    "model_correct = sm.OLS(Y, XW_with_const).fit()\n",
    "print(\"\\nCorrect model (with W):\")\n",
    "print(model_correct.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e3979",
   "metadata": {},
   "source": [
    "In this example, ğ‘‹ and ğ‘Š are positively correlated, and both increase ğ‘Œ. When you leave out ğ‘Š, the model attributes part of ğ‘Šâ€™s influence on ğ‘Œ to ğ‘‹, overestimating the effect of ğ‘‹.\n",
    "\n",
    "If instead:\n",
    "\n",
    "- ğ›½2 < 0 (negative effect of ğ‘Š on ğ‘Œ), or\n",
    "- corr(ğ‘‹, ğ‘Š) < 0 (negative correlation),\n",
    "\n",
    "then the bias direction could flip, leading to underestimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c445ba2",
   "metadata": {},
   "source": [
    "2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.\n",
    "\n",
    "W = [noise]\n",
    "X = [noise]\n",
    "Y = 2 * X + [noise]\n",
    "\n",
    "And compute the p-value of a coefficient - in this case, the coefficient of W.  \n",
    "(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)\n",
    "If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)\n",
    "Run the analysis 1000 times and report the best (smallest) p-value.  \n",
    "If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761bda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum p-value found: 0.0004745704124689032\n",
      "Number of runs with p-value < 0.05: 51 out of 1000\n",
      "         p_value_W\n",
      "count  1000.000000\n",
      "mean      0.495585\n",
      "std       0.286279\n",
      "min       0.000475\n",
      "25%       0.244707\n",
      "50%       0.504390\n",
      "75%       0.739878\n",
      "max       0.999762\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_samples = 100         # number of data points in each simulation\n",
    "n_simulations = 1000    # number of repeated simulations\n",
    "\n",
    "# Store minimum p-value found across simulations\n",
    "min_p_value = 1.0\n",
    "\n",
    "# Store all p-values for summary\n",
    "all_p_values = []\n",
    "\n",
    "# Run in a loop\n",
    "for _ in range(n_simulations):\n",
    "    # Generate random noise variables\n",
    "    W = np.random.normal(0, 1, n_samples)\n",
    "    X = np.random.normal(0, 1, n_samples)\n",
    "    noise = np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Generate Y where only X matters (coefficient for W is truly zero)\n",
    "    Y = 2 * X + noise\n",
    "\n",
    "    # Create design matrix with intercept, W, and X\n",
    "    X_design = sm.add_constant(np.column_stack((W, X)))\n",
    "    \n",
    "    # Fit linear regression model\n",
    "    model = sm.OLS(Y, X_design).fit()\n",
    "    \n",
    "    # Get p-value for W (index 1: [const, W, X])\n",
    "    p_value_W = model.pvalues[1]\n",
    "    all_p_values.append(p_value_W)\n",
    "    \n",
    "    # Update minimum p-value if smaller one is found\n",
    "    if p_value_W < min_p_value:\n",
    "        min_p_value = p_value_W\n",
    "\n",
    "# Convert p-values to DataFrame for summary or export\n",
    "summary_df = pd.DataFrame({'p_value_W': all_p_values})\n",
    "\n",
    "# Count how many times p-value was below 0.05\n",
    "significant_count = (summary_df['p_value_W'] < 0.05).sum()\n",
    "\n",
    "# Report results\n",
    "print(f\"Minimum p-value found: {min_p_value}\")\n",
    "print(f\"Number of runs with p-value < 0.05: {significant_count} out of {n_simulations}\")\n",
    "\n",
    "# Optional: show p-value summary table\n",
    "print(summary_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1f1ab",
   "metadata": {},
   "source": [
    "A p-value below 0.05 does not guarantee the coefficient is nonzero. It just means that, if the true coefficient were zero, weâ€™d observe such an extreme estimate about 5% of the time by random chance.\n",
    "\n",
    "By running the analysis 1,000 times and picking the smallest p-value, you artificially inflate the chance of finding a â€œsignificantâ€ result this is called p-hacking. The more tests you run, the more likely you are to get a small p-value by chance, even when no true effect exists. So, finding a p-value < 0.05 after many repeated trials does not imply a real effect. It often just reflects statistical noise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
